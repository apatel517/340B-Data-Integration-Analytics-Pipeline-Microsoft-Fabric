{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, to_date, col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# ------------------------------------\n",
    "# 1. Claims Data\n",
    "# ------------------------------------\n",
    "claims_path = \"abfss://fakepath/Files/RawData/Claims_Data/*.csv\"\n",
    "df_claims = spark.read.option(\"header\", True).csv(claims_path)\n",
    "df_claims = (\n",
    "    df_claims\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"payment_amount\", col(\"payment_amount\").cast(DoubleType()))\n",
    "    .withColumn(\"processed_date\", to_date(\"processed_date\", \"yyyy-MM-dd\"))\n",
    ")\n",
    "df_claims.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze_claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eaf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# 2. Prescriptions Data\n",
    "# ------------------------------------\n",
    "prescriptions_path = \"abfss:/fakepath/Files/RawData/Prescriptions_Data/*.csv\"\n",
    "df_prescriptions = spark.read.option(\"header\", True).csv(prescriptions_path)\n",
    "df_prescriptions = (\n",
    "    df_prescriptions\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"rx_date\", to_date(\"rx_date\", \"yyyy-MM-dd\"))\n",
    ")\n",
    "df_prescriptions.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze_prescriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# 3. Dispensing Data\n",
    "# ------------------------------------\n",
    "dispensing_path = \"abfss://fakepath/Files/RawData/Dispensing_Data/*.csv\"\n",
    "df_dispensing = spark.read.option(\"header\", True).csv(dispensing_path)\n",
    "df_dispensing = (\n",
    "    df_dispensing\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"fill_date\", to_date(\"fill_date\", \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\n",
    "    .withColumn(\"patient_pay\", col(\"patient_pay\").cast(DoubleType()))\n",
    "    .withColumn(\"third_party_paid\", col(\"third_party_paid\").cast(DoubleType()))\n",
    ")\n",
    "df_dispensing.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze_dispensing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# 4. Inventory Data\n",
    "# ------------------------------------\n",
    "inventory_path = \"abfss://fakepath/Files/RawData/Inventory_Data/*.csv\"\n",
    "df_inventory = spark.read.option(\"header\", True).csv(inventory_path)\n",
    "df_inventory = (\n",
    "    df_inventory\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"order_date\", to_date(\"order_date\", \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"received_date\", to_date(\"received_date\", \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"quantity_ordered\", col(\"quantity_ordered\").cast(IntegerType()))\n",
    "    .withColumn(\"quantity_received\", col(\"quantity_received\").cast(IntegerType()))\n",
    ")\n",
    "df_inventory.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze_inventory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# 5. Patient Eligibility Data\n",
    "# ------------------------------------\n",
    "patient_eligibility_path = \"abfss:/fakepath/Files/RawData/Patient_Data/*.csv\"\n",
    "df_patient_eligibility = spark.read.option(\"header\", True).csv(patient_eligibility_path)\n",
    "df_patient_eligibility = (\n",
    "    df_patient_eligibility\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"encounter_date\", to_date(\"encounter_date\", \"yyyy-MM-dd\"))\n",
    ")\n",
    "df_patient_eligibility.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze_patient_eligibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# 6. Drug Catalog - Append only new files based on source_file\n",
    "# ------------------------------------\n",
    "drug_catalog_path = \"abfss://fakepath/Files/RawData/Drug_catalog/*.csv\"\n",
    "df_drug_new = spark.read.option(\"header\", True).csv(drug_catalog_path).withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "try:\n",
    "    # Read distinct source_file names already loaded\n",
    "    df_existing_sources = spark.read.table(\"bronze_drug_catalog\").select(\"source_file\").distinct()\n",
    "    \n",
    "    # Filter out the already loaded files from new dataframe\n",
    "    df_drug_filtered = df_drug_new.join(df_existing_sources, on=\"source_file\", how=\"left_anti\")\n",
    "except Exception as e:\n",
    "    # If bronze_drug_catalog doesn't exist yet, load all\n",
    "    print(f\"Could not read existing bronze_drug_catalog table: {e}\")\n",
    "    df_drug_filtered = df_drug_new\n",
    "\n",
    "if df_drug_filtered.count() > 0:\n",
    "    df_drug_catalog = (\n",
    "        df_drug_filtered\n",
    "        .withColumn(\"340b_price\", col(\"340b_price\").cast(DoubleType()))\n",
    "        .withColumn(\"wac_price\", col(\"wac_price\").cast(DoubleType()))\n",
    "        .withColumn(\"gpo_price\", col(\"gpo_price\").cast(DoubleType()))\n",
    "    )\n",
    "    df_drug_catalog.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze_drug_catalog\")\n",
    "    display(df_drug_catalog)\n",
    "else:\n",
    "    print(\"No new drug catalog files to process.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}